{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', 'A', 'D', 'S', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "vocabulary size:  28\n"
     ]
    }
   ],
   "source": [
    "text = 'During working in the company you will be faced with different computer vision tasks. So it is useful to have skills in image segmentation. After completing this test task you will be able to implement similar algorithms in commercial projects.'\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print('vocabulary size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3, 24, 21, 13, 18, 11,  0, 26, 19, 21, 15, 13, 18, 11,  0, 13, 18,  0,\n",
      "        23, 12,  9,  0,  7, 19, 17, 20,  5, 18, 27,  0, 27, 19, 24,  0, 26, 13,\n",
      "        16, 16,  0,  6,  9,  0, 10,  5,  7,  9,  8,  0, 26, 13])\n"
     ]
    }
   ],
   "source": [
    "string2int = {ch:i for i,ch in enumerate(chars)}\n",
    "int2string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [string2int[c] for c in x]\n",
    "decode = lambda x: ''.join([int2string[c] for c in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[185, 134,  28, 142]])\n"
     ]
    }
   ],
   "source": [
    "ix = torch.randint(len(data) - block_size, (1, batch_size))\n",
    "print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[16,  5, 21,  0,  5, 16, 11, 19],\n",
      "        [19, 17, 17,  9, 21,  7, 13,  5],\n",
      "        [17, 20, 16,  9, 17,  9, 18, 23],\n",
      "        [ 0, 27, 19, 24,  0, 26, 13, 16]])\n",
      "target:  tensor([[ 5, 21,  0,  5, 16, 11, 19, 21],\n",
      "        [17, 17,  9, 21,  7, 13,  5, 16],\n",
      "        [20, 16,  9, 17,  9, 18, 23,  0],\n",
      "        [27, 19, 24,  0, 26, 13, 16, 16]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "print(\"input: \", x)\n",
    "y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "print(\"target: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tlnicadmn..ji .rngrbpduutktfvcaptfjmgedafbfapsaifADDmgthngevrdm sAoo blfrbmSduapSuDdehpdirSvAmgn.plStvhirgeshpowADepiADSvar tlrkaumjdmohdknrrumutap.hdasrknrutlrDfDyumAajAeylmnAblyyutmibp.uuutlop utlsdwueslrhhhdknggautlg.DykvmirAsc.Dgs dopryyuuDyDgvvne..dkdblDSshrsp.DfpyuDutlrkwcfvwby tcam by.ykofAvkDenrfAt.gruuumAvlaspduuujykpseaprfestk ruutlwAjoblaumidvsyng.Delr .yA.uDycok.ppvhry.ycsprkaSk.ynge dvlrirkaggrAnbjSggngjyS t.ADit v ngykap.dg.Dejhhlwriblwai.gvkiuutjpefp.aruuuu birbmiruap tlrwh.Sussjo\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.token_embed_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embed_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_charts = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[21,  0, 25, 13, 22, 13, 19, 18],\n",
      "        [15, 22,  1,  0,  4, 19,  0, 13],\n",
      "        [ 9,  0, 10,  5,  7,  9,  8,  0],\n",
      "        [13, 23, 12,  0,  8, 13, 10, 10]])\n",
      "targets:\n",
      "tensor([[ 0, 25, 13, 22, 13, 19, 18,  0],\n",
      "        [22,  1,  0,  4, 19,  0, 13, 23],\n",
      "        [ 0, 10,  5,  7,  9,  8,  0, 26],\n",
      "        [23, 12,  0,  8, 13, 10, 10,  9]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.778, val loss: 3.860\n",
      "step: 250, train loss: 3.677, val loss: 3.815\n",
      "step: 500, train loss: 3.622, val loss: 3.787\n",
      "step: 750, train loss: 3.555, val loss: 3.755\n",
      "3.36236572265625\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D radmlulwAknD tkspotautruuejrSuuuhapgoohdspngglybsyutknhwpi..hsDSumcynveobp sapdvfnvAerluutlluututap  Avlra.yknvo tajow p.jyDjhpdbfDy.Drutloyornlykplut.hAlntljrdbwtcouu sjnrbpigvAAbltDkccwAhps.hhhwitbmg tlvfwnDwhefvu soofvdmykwmncspjplncspi.kplwpei s.DwapSkAmf uDnrSjmapDfpirdibdnvnr Saowb.dessaAmi.yuauyjlp jljhnkhocfapykvjircail uamvobbAvrjajm.Dy.Du tlykibjblraulcitagorcowADvbp dnvfbpyoavnuDdwtrnutkesDamnvsauuutejmgnrjrhru.DrhnhespkygpduSanrhgnS jruutlSorr agsdu.dkchdbcrkpvocviyumglaSkgp tlw sp\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Pre-trained Transformer (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-exploration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
